{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying information from a Website\n",
    "<br><br>\n",
    "Identifying basic technologies used by a website using the `buildwith` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtwith\n",
    "\n",
    "\n",
    "tecs = builtwith.parse('https://www.facebook.com')\n",
    "\n",
    "for tec, value in tecs.items():\n",
    "    for i in value:\n",
    "        print(\"-\", tec, \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the owner of a website. \n",
    "<br>\n",
    "If a company is known for blocking web crawlers, it would be good to be more conservative with the download rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "\n",
    "\n",
    "owner_information = whois.whois('facebook.com')\n",
    "\n",
    "for information, key in owner_information.items():\n",
    "    print(\"-\", information, \":\", key, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating content data read from a page's HTML output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `BeautifulSoup` to perform the analysis of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "html = urlopen(\"http://google.com\")\n",
    "bsObj = BeautifulSoup(html.read(), \"html.parser\")\n",
    "\n",
    "\n",
    "page_links = bsObj.find_all('a')\n",
    "\n",
    "for link in page_links: \n",
    "    print(\"-\", link.get(\"href\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most complete program that searches for links from any website\n",
    "\n",
    "Scraping news based on names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "pages = set()\n",
    "invalid_pages = set()\n",
    "new_page = \"\"\n",
    "\n",
    "def open_page(url_page):\n",
    "    global pages\n",
    "    try:\n",
    "        if url_page not in invalid_pages:\n",
    "            html = urlopen(url_page)\n",
    "            bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "            regex_keys = ('.bolsonaro.|.russia.|.ucrania.')\n",
    "            search = bsObj.findAll(\"a\", href=re.compile(regex_keys))\n",
    "\n",
    "            for link in search:\n",
    "                if \"href\" in link.attrs:\n",
    "                    if link.attrs['href'] not in pages and link.attrs['href'] not in invalid_pages:\n",
    "                        new_page = link.attrs['href']\n",
    "                        print(new_page)\n",
    "                        pages.add(new_page)\n",
    "                        open_page(new_page)\n",
    "    except:\n",
    "        invalid_pages.add(new_page)\n",
    "\n",
    "open_page(\"http://g1.globo.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target verification:  scan script \n",
    "<br>\n",
    "\n",
    "The socket has an option called `gethostbyname`, which one of the two is indifferent, we can use both. `gethostbyname_ex` allows us to display more information in case the target, for example, the target has one more server that does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gethostbyname` is the simplest as it takes the host from your name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket \n",
    "\n",
    "\n",
    "domain = input(\"Target: \")\n",
    "\n",
    "with open('notes/wordlist.txt', 'r') as files:\n",
    "    brute_force = files.readlines()\n",
    "\n",
    "for name in brute_force:\n",
    "    DNS = name.strip(\"\\n\") + \".\" + domain\n",
    "    try:\n",
    "        print(\"-\" + DNS + \": \" + socket.gethostbyname(DNS)) # get the host from the name\n",
    "    except socket.gaierror:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going to DNS Resolver\n",
    "<br>\n",
    "We will use query (determine whether each record type exists or not), which will be responsible for getting the information we want to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dns.resolver\n",
    "\n",
    "\n",
    "domain = input(\"Target: \")\n",
    "registers = [\"AAAA\", \"A\", \"MX\", \"NS\"]\n",
    "\n",
    "for register in registers:\n",
    "    result = dns.resolver.resolve(domain, register, raise_on_no_answer = False)\n",
    "    if result.rrset is not None:\n",
    "        print(\"-\", result.rrset)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
